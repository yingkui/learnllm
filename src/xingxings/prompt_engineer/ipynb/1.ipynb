{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set API key and make basic requests with the client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a haiku about a pet chicken:\n",
      "\n",
      "Feathered friend clucks near,\n",
      "Gathering crumbs from the ground,\n",
      "Loyal, fluffy pet.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "my_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "client = Anthropic(\n",
    "    api_key=my_api_key\n",
    ")\n",
    "\n",
    "our_first_message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=4096,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hi there! Please write me a haiku about a pet chicken\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(our_first_message.content[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The returned object contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `id` - a unique object identifier\n",
    "   \n",
    "2. `content` - model generated content\n",
    "\n",
    "3. `type` - The object type, which will always be \"message\"\n",
    "   \n",
    "4. `role` - The conversational role of the generated message. This will always be \"assistant\".\n",
    "   \n",
    "5. `model` - The model that handled the request and generated the response\n",
    "   \n",
    "6. `stop_reason` - The reason the model stopped generating.  We'll learn more about this later.\n",
    "   \n",
    "7. `stop_sequence` - We'll learn more about this shortly.\n",
    "   \n",
    "8.  `usage` - information on billing and rate-limit usage. Contains information on:\n",
    "    * `input_tokens` - The number of input tokens that were used.\n",
    "  \n",
    "    * `output_tokens` - The number of output tokens that were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_011Bwfte1GxBqZHML2b3Yh4T', content=[TextBlock(text=\"Here's a haiku about a pet chicken:\\n\\nFeathers soft and bright,\\nClucking cheerfully all day,\\nMy loyal chicken.\", type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=20, output_tokens=36))\n"
     ]
    }
   ],
   "source": [
    "print(our_first_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有用的就生成的 content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameters of client.messages.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. `model` - str, 必须\n",
    "   \n",
    "  2. `messages` - List[Dict[str, str]], 必填，重点内容\n",
    "   \n",
    "  3. `max_tokens` - int, 必填\n",
    "   \n",
    "  4. `temperature` - Optional[float] = None, 调整随机性\n",
    "   \n",
    "  5. `tools` -  可以选择调动的工具\n",
    "   \n",
    "  6. `tool_choice` - 工具选择方式  \n",
    "   \n",
    "  7. `top_p` - Optional[float] = None, 调整随机性\n",
    "   \n",
    "  8.  `top_k` - Optional[int] = None, 调整随机性\n",
    "   \n",
    "  9.  `stop_sequences` - Optional[List[str]] = None,\n",
    "   \n",
    "  10. `stream` - Optional[bool] = None, steam 生成\n",
    "   \n",
    "  11. `metadata` - Optional[Dict[str, str]] = None, 放置一些定位信息，保证上下文，比如同用一个 API 的条件下，多个租户同时调用\n",
    "   \n",
    "  12. `system` - Optional[str] = None\n",
    "   \n",
    "  13. `prefill` - Optional[str] = None, 回答的前缀  \n",
    "   \n",
    "  14. `**kwargs` - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "  max_tokens:\n",
      "    Type: int\n",
      "    Default: <class 'inspect._empty'>\n",
      "\n",
      "  messages:\n",
      "    Type: Iterable[MessageParam]\n",
      "    Default: <class 'inspect._empty'>\n",
      "\n",
      "  model:\n",
      "    Type: ModelParam\n",
      "    Default: <class 'inspect._empty'>\n",
      "\n",
      "  metadata:\n",
      "    Type: MetadataParam | NotGiven\n",
      "    Default: NOT_GIVEN\n",
      "\n",
      "  stop_sequences:\n",
      "    Type: List[str] | NotGiven\n",
      "    Default: NOT_GIVEN\n",
      "\n",
      "  stream:\n",
      "    Type: Literal[False] | Literal[True] | NotGiven\n",
      "    Default: NOT_GIVEN\n",
      "\n",
      "  system:\n",
      "    Type: Union[str, Iterable[TextBlockParam]] | NotGiven\n",
      "    Default: NOT_GIVEN\n",
      "\n",
      "  temperature:\n",
      "    Type: float | NotGiven\n",
      "    Default: NOT_GIVEN\n",
      "\n",
      "  tool_choice:\n",
      "    Type: ToolChoiceParam | NotGiven\n",
      "    Default: NOT_GIVEN\n",
      "\n",
      "  tools:\n",
      "    Type: Iterable[ToolParam] | NotGiven\n",
      "    Default: NOT_GIVEN\n",
      "\n",
      "  top_k:\n",
      "    Type: int | NotGiven\n",
      "    Default: NOT_GIVEN\n",
      "\n",
      "  top_p:\n",
      "    Type: float | NotGiven\n",
      "    Default: NOT_GIVEN\n",
      "\n",
      "  extra_headers:\n",
      "    Type: Headers | None\n",
      "    Default: None\n",
      "\n",
      "  extra_query:\n",
      "    Type: Query | None\n",
      "    Default: None\n",
      "\n",
      "  extra_body:\n",
      "    Type: Body | None\n",
      "    Default: None\n",
      "\n",
      "  timeout:\n",
      "    Type: float | httpx.Timeout | None | NotGiven\n",
      "    Default: NOT_GIVEN\n",
      "\n",
      "Help on method create in module anthropic.resources.messages.messages:\n",
      "\n",
      "create(*, max_tokens: 'int', messages: 'Iterable[MessageParam]', model: 'ModelParam', metadata: 'MetadataParam | NotGiven' = NOT_GIVEN, stop_sequences: 'List[str] | NotGiven' = NOT_GIVEN, stream: 'Literal[False] | Literal[True] | NotGiven' = NOT_GIVEN, system: 'Union[str, Iterable[TextBlockParam]] | NotGiven' = NOT_GIVEN, temperature: 'float | NotGiven' = NOT_GIVEN, tool_choice: 'ToolChoiceParam | NotGiven' = NOT_GIVEN, tools: 'Iterable[ToolParam] | NotGiven' = NOT_GIVEN, top_k: 'int | NotGiven' = NOT_GIVEN, top_p: 'float | NotGiven' = NOT_GIVEN, extra_headers: 'Headers | None' = None, extra_query: 'Query | None' = None, extra_body: 'Body | None' = None, timeout: 'float | httpx.Timeout | None | NotGiven' = NOT_GIVEN) -> 'Message | Stream[RawMessageStreamEvent]' method of anthropic.resources.messages.messages.Messages instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "# 获取 client.messages.create 方法的参数信息\n",
    "parameters = inspect.signature(client.messages.create).parameters\n",
    "\n",
    "# 打印参数信息\n",
    "print(\"Parameters:\")\n",
    "for name, param in parameters.items():\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Type: {param.annotation}\")\n",
    "    print(f\"    Default: {param.default}\")\n",
    "    print()\n",
    "\n",
    "# 打印文档的 help\n",
    "help(client.messages.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature，Top-p, Top-k solve the randomness in mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `temperature` - 每一步生成过程中，可能的词，做成一个概率分布图，temperature 调整这个分布是更集中还是更平坦，它也能略微治一治幻觉\n",
    "   \n",
    "2. `top-k` - 选词的空间，从高到低，按照个数算\n",
    "   \n",
    "3. `top-p` - 也是选词的空间，从高到低，按照概率和达到一定值来算\n",
    "   \n",
    "4. 数学性的任务，需要更多的确定性，如果能保证 AI 生成的答案符合要求，就用这它们锁定答案内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在锐角三角形 ABC 中,给定 $a = 2b \\sin A$,我们需要求 $\\cos A + \\sin C$ 的取值范围。\n",
      "\n",
      "首先,我们可以利用三角形的基本关系式:\n",
      "$a^2 = b^2 + c^2 - 2bc \\cos A$\n",
      "\n",
      "将 $a = 2b \\sin A$ 代入,可得:\n",
      "$4b^2 \\sin^2 A = b^2 + c^2 - 2bc \\cos A$\n",
      "$4 \\sin^2 A = 1 + \\frac{c^2}{b^2} - 2 \\cos A$\n",
      "\n",
      "整理得:\n",
      "$\\cos A = \\frac{1 + \\frac{c^2}{b^2} - 4 \\sin^2 A}{2}$\n",
      "\n",
      "由于三角形 ABC 是锐角三角形,因此 $\\sin C = \\sqrt{1 - \\cos^2 C}$。将上式代入,可得:\n",
      "$\\cos A + \\sin C = \\frac{1 + \\frac{c^2}{b^2} - 4 \\sin^2 A}{2} + \\sqrt{1 - \\left(\\frac{1 + \\frac{c^2}{b^2} - 4 \\sin^2 A}{2}\\right)^2}$\n",
      "\n",
      "这个表达式的取值范围取决于 $\\sin A$ 的取值范围,即 $0 < \\sin A < 1$。\n",
      "\n",
      "因此,$\\cos A + \\sin C$ 的取值范围为:\n",
      "$\\frac{1 + \\frac{c^2}{b^2}}{2} \\le \\cos A + \\sin C \\le \\frac{1 + \\frac{c^2}{b^2}}{2} + \\sqrt{1 - \\left(\\frac{1 + \\frac{c^2}{b^2}}{2}\\right)^2}$\n"
     ]
    }
   ],
   "source": [
    "# Get Claude's response\n",
    "response = client.messages.create(\n",
    "      model=\"claude-3-haiku-20240307\",\n",
    "      max_tokens=4096,\n",
    "      messages=[\n",
    "          {\"role\": \"user\", \"content\": \"在锐角 $\\triangle ABC$ 中，若 $a = 2b \\sin A$，求 $\\cos A + \\sin C$ 的取值范围\"}\n",
    "      ],\n",
    "      temperature=0.1,\n",
    "      top_p=0.5,\n",
    "      top_k=20,\n",
    "    )\n",
    "\n",
    "# Print Claude's response\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the first generation："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在锐角三角形 ABC 中,给定 $a = 2b \\sin A$,我们需要求 $\\cos A + \\sin C$ 的取值范围。\n",
    "\n",
    "首先,我们可以利用三角形的基本关系式:\n",
    "$a^2 = b^2 + c^2 - 2bc \\cos A$\n",
    "\n",
    "将 $a = 2b \\sin A$ 代入,可得:\n",
    "$4b^2 \\sin^2 A = b^2 + c^2 - 2bc \\cos A$\n",
    "$4 \\sin^2 A = 1 + \\frac{c^2}{b^2} - 2 \\cos A$\n",
    "\n",
    "整理得:\n",
    "$\\cos A = \\frac{1 + \\frac{c^2}{b^2} - 4 \\sin^2 A}{2}$\n",
    "\n",
    "由于三角形 ABC 是锐角三角形,因此 $\\sin C = \\sqrt{1 - \\cos^2 C}$。将上式代入,可得:\n",
    "$\\cos A + \\sin C = \\frac{1 + \\frac{c^2}{b^2} - 4 \\sin^2 A}{2} + \\sqrt{1 - \\left(\\frac{1 + \\frac{c^2}{b^2} - 4 \\sin^2 A}{2}\\right)^2}$\n",
    "\n",
    "这个表达式的取值范围取决于 $\\sin A$ 的取值范围,即 $0 < \\sin A < 1$。\n",
    "\n",
    "因此,$\\cos A + \\sin C$ 的取值范围为:\n",
    "$\\frac{1 + \\frac{c^2}{b^2}}{2} \\le \\cos A + \\sin C \\le 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the second generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在锐角三角形 ABC 中,给定 $a = 2b \\sin A$,我们需要求 $\\cos A + \\sin C$ 的取值范围。\n",
    "\n",
    "首先,我们可以利用三角形的基本关系式:\n",
    "$a^2 = b^2 + c^2 - 2bc \\cos A$\n",
    "\n",
    "将 $a = 2b \\sin A$ 代入,可得:\n",
    "$4b^2 \\sin^2 A = b^2 + c^2 - 2bc \\cos A$\n",
    "$4 \\sin^2 A = 1 + \\frac{c^2}{b^2} - 2 \\cos A$\n",
    "\n",
    "整理得:\n",
    "$\\cos A = \\frac{1 + \\frac{c^2}{b^2} - 4 \\sin^2 A}{2}$\n",
    "\n",
    "由于三角形 ABC 是锐角三角形,因此 $\\sin C = \\sqrt{1 - \\cos^2 C}$。将上式代入,可得:\n",
    "$\\cos A + \\sin C = \\frac{1 + \\frac{c^2}{b^2} - 4 \\sin^2 A}{2} + \\sqrt{1 - \\left(\\frac{1 + \\frac{c^2}{b^2} - 4 \\sin^2 A}{2}\\right)^2}$\n",
    "\n",
    "这个表达式的取值范围取决于 $\\sin A$ 的取值范围,即 $0 < \\sin A < 1$。\n",
    "\n",
    "因此,$\\cos A + \\sin C$ 的取值范围为:\n",
    "$\\frac{1 + \\frac{c^2}{b^2}}{2} \\le \\cos A + \\sin C \\le \\frac{1 + \\frac{c^2}{b^2}}{2} + \\sqrt{1 - \\left(\\frac{1 + \\frac{c^2}{b^2}}{2}\\right)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS. **大模型建立在统计学之上，这种调整影响很大**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The use of 'tool'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. 模型可以根据内容选择调用设定好的函数，返回调用状态，外部人写函数尽心处理后，返回给 AI，AI 接受信息，纳入回答的范围，举一个模型调用天气报告函数\n",
    "   \n",
    "2. 这个过程以手写函数为主体，而 AI 有点像一个函数在被调用\n",
    "   \n",
    "3. 可以用函数 trigger AI，让 AI 接受一些信息，生成回答，返回给学生，比如调用用户的做题信息，分析后给出学习建议\n",
    "   \n",
    "    PS. chatgpt和 deepseek 的回答的信息是过时的，claude 把 response 的数据结构改了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义工具函数\n",
    "def get_weather(city):\n",
    "    weather_data = {\n",
    "        \"北京\": \"晴天，温度25°C\",\n",
    "        \"上海\": \"小雨，温度20°C\"\n",
    "    }\n",
    "    return weather_data.get(city, \"未知城市天气信息\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的,我很乐意为您提供北京当前的天气信息和相应的穿衣建议。\n",
      "\n",
      "根据最新的天气预报,北京今天天气晴好,温度在25度左右。这样的天气属于春秋过渡时期,既不太寒冷也不太炎热,非常适合外出活动。\n",
      "\n",
      "我的穿衣建议如下:\n",
      "\n",
      "1. 可以选择一件薄外套或者长袖衬衫作为上装。由于中午时气温较高,穿厚重的衣服可能会觉得有些闷热。\n",
      "\n",
      "2. 选择轻薄的长裤或者七分裤都可以。这样既能遮住腿部,又不会觉得太热。\n",
      "\n",
      "3. 最好准备一件薄款外套或者披肩,以防日落后气温略有降低。\n",
      "\n",
      "4. 建议选择透气性好的鞋子,如运动鞋或者皮鞋都可以。\n",
      "\n",
      "总的来说,今天的天气比较凉爽宜人,适合穿淡色系的轻薄服饰,在户外活动也不会感到太冷或太热。请根据这些建议选择合适的服装。如果还有任何其他需要,欢迎随时告诉我。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 调用 Claude API\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=4096,  # 确保 max_tokens 足够大\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"请告诉我北京的天气如何？并给我提供穿衣建议\"}\n",
    "    ],\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"提供指定城市的天气信息\",\n",
    "            \"input_schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"要查询天气的城市名称\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"city\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 解析工具调用\n",
    "if response.content and response.content[0].type == 'tool_use':  # 检查是否有工具调用\n",
    "    tool_call = response.content[0]\n",
    "    if tool_call.name == 'get_weather':  # 检查工具名称\n",
    "        city = tool_call.input['city']  # 提取参数\n",
    "        weather_info = get_weather(city)  # 调用工具函数\n",
    "        \n",
    "        # 将工具结果传回 Claude\n",
    "        final_response = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=4096,  # 确保 max_tokens 足够大\n",
    "            messages=[\n",
    "                {\"role\": \"assistant\", \"content\": f\"北京的天气是 {weather_info}\"}, # 注意，之前已经 user prompt，此时应该是 assistant response，否则回答会出现半截的问题\n",
    "                {\"role\": \"user\", \"content\": \"请告诉我北京的天气如何？并给我提供穿衣建议\"},\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 打印完整响应内容\n",
    "        if final_response.content:\n",
    "            for content_block in final_response.content:\n",
    "                if content_block.type == 'text':\n",
    "                    print(content_block.text)\n",
    "        else:\n",
    "            print(\"未收到有效响应。\")\n",
    "else:\n",
    "    print(\"没有工具调用。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS. **调用函数还是不够方便，直接在会话能调用，效率就完全不一样了**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 定义模型的角色，让模型的回答有现实情景，能够保值一致，比如你是一个数学老师\n",
    "   \n",
    "2. 定义回答的规则，让模型有规则可以依赖，比如请不要回答与数学无关的问题，并客气的拒绝学生\n",
    "   \n",
    "3. system prompt 放到 messages 之外，是提供全局指示的，messages 里只能作用于当前会话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.11 和 9.9 进行比较,可以得出以下结果:\n",
      "\n",
      "9.11 > 9.9\n",
      "\n",
      "原因是:\n",
      "\n",
      "1. 两个数字都是小数,比较时需要逐位比较。\n",
      "2. 首位数字9是相同的,所以需要比较第二位数字。\n",
      "3. 9.11中的第二位数字是1,而9.9中的第二位数字是0,1 > 0,所以9.11更大。\n",
      "\n",
      "因此,9.11比9.9更大。\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "      model=\"claude-3-haiku-20240307\",\n",
    "      max_tokens=4096,\n",
    "    #   system=\"你是一个数学老师，请按照数学知识回答学生的问题\",\n",
    "      messages=[\n",
    "          {\"role\": \"user\", \"content\": \"9.11和 9.9，哪个更大\"}\n",
    "      ],\n",
    "      temperature=0.1,\n",
    "      top_p=0.5,\n",
    "      top_k=20,\n",
    "    )\n",
    "\n",
    "# Print Claude's response\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据数学知识,9.11和9.9进行大小比较,9.11更大。\n",
      "\n",
      "原因如下:\n",
      "\n",
      "1. 9.11和9.9都是小数,比较大小时需要逐位比较。\n",
      "\n",
      "2. 首位数字9是相同的,所以需要比较第二位小数。\n",
      "\n",
      "3. 9.11的第二位小数是1,而9.9的第二位小数是0,1大于0,因此9.11大于9.9。\n",
      "\n",
      "所以,根据数学知识,9.11这个数字更大。\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "      model=\"claude-3-haiku-20240307\",\n",
    "      max_tokens=4096,\n",
    "      system=\"你是一个数学老师，请按照数学知识回答学生的问题\",\n",
    "      messages=[\n",
    "          {\"role\": \"user\", \"content\": \"9.11和 9.9，哪个更大\"}\n",
    "      ],\n",
    "      temperature=0.1,\n",
    "      top_p=0.5,\n",
    "      top_k=20,\n",
    "    )\n",
    "\n",
    "# Print Claude's response\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS. **理解 AI 一个重要的方面，就是去理解训练它的数据，它的行为就是被数据所决定的**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The prompt of messages （超级重点）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prompt 中包含的要素：role, task, tone, data, example, details, step-by-step, outputformatting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关键信息\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination 如何避免\n",
    "\n",
    "1. 利用 temperature， tok-p 和 top-k，设定更小的值\n",
    "2. 在 prompt 的表达上，允许大模型在不确定答案说不知道\n",
    "3. 在 prompt 中要求大模型回答时写出求证的过程\n",
    "4. 在 prompt 中提供明确的思路和工具\n",
    "5. 进行验证，比如自己再次生成答案对比，call 外部的大模型得到答案进行对比，对比现实中已经有的一些结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
